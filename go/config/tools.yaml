# Wilson Assistant Configuration File

workspace:
  # Workspace path - where Wilson can access files
  # Options:
  #   - Leave empty or comment out to use user's home directory (~/)
  #   - Use "~" or "~/" for user's home directory
  #   - Use "~/Documents" for a specific folder in home
  #   - Use absolute path like "/path/to/workspace"
  #   - Set WILSON_WORKSPACE environment variable to override
  path: "~"

ollama:
  model: "llama3:latest"
  url: "http://localhost:11434"

# Multi-LLM Configuration
# Choose models based on your available RAM (see README.md for full guidance):
#
# Low-End (8GB RAM):
#   chat: qwen2.5:3b (2GB)        - Good tool calling, basic conversation
#   analysis: qwen2.5:3b (2GB)    - Decent analysis
#   code: qwen2.5:7b (4GB)        - Smaller code model
#
# Mid-Range (16GB RAM) - RECOMMENDED:
#   chat: qwen2.5:7b (4GB)        - Excellent tool calling, good conversation
#   analysis: qwen2.5:7b (4GB)    - Strong analysis
#   code: qwen2.5-coder:14b (8GB) - Professional code generation
#
# High-End (32GB+ RAM):
#   chat: qwen2.5:7b (4GB)        - Fast chat
#   analysis: qwen2.5:14b (8GB)   - Deep analysis
#   code: qwen2.5-coder:32b (16GB)- Production-grade code
#
# Why qwen2.5? Better structured output (tool calling) than llama3 at any size

llms:
  chat:
    provider: "ollama"
    model: "qwen2.5:7b"  # Chat agent - always loaded, handles tool calling
    temperature: 0.7
    base_url: "http://localhost:11434"

  analysis:
    provider: "ollama"
    model: "qwen2.5:7b"  # Research/analysis tasks
    temperature: 0.3
    base_url: "http://localhost:11434"

  code:
    provider: "ollama"
    model: "qwen2.5-coder:14b"  # Code generation worker (ephemeral)
    temperature: 0.1
    base_url: "http://localhost:11434"

tools:
  # Filesystem Tools
  list_files:
    enabled: true
    requires_confirm: false

  read_file:
    enabled: true
    requires_confirm: false
    max_file_size: 10000  # Max bytes to read

  search_files:
    enabled: true
    requires_confirm: false

  # System Tools
  run_command:
    enabled: true
    requires_confirm: true  # Always confirm before running commands
    timeout: 30s
    blocked_patterns:
      - "rm -rf"
      - "sudo"
      - "curl"
      - "wget"
      - "dd"
      - "mkfs"
      - "> /dev"

  # Web Tools
  search_web:
    enabled: true
    requires_confirm: false
    max_results: 10

  fetch_page:
    enabled: true
    requires_confirm: true
    timeout: 30s
    max_file_size: 5242880  # 5MB
    allowed_domains:
      - "github.com"
      - "*.github.com"
      - "stackoverflow.com"
      - "*.stackoverflow.com"
      - "docs.python.org"
      - "go.dev"
      - "pkg.go.dev"

  extract_content:
    enabled: true
    requires_confirm: false

  analyze_content:
    enabled: true
    requires_confirm: false
    llm: "analysis"  # Uses the analysis LLM (mixtral)
    max_content_length: 10000
    prompts:
      summarize: "Provide a concise summary of the following content in 3-5 bullet points. Focus on the key information and main takeaways."
      extract_key_points: "Extract and list the main points, important facts, and key information from the following content."
      answer_question: "Based on the following content, answer this question: {question}"

audit:
  enabled: true
  log_path: ".wilson/audit.log"
  log_level: "info"  # info, warning, error

# Context Store Configuration
context:
  enabled: true
  db_path: ".wilson/memory.db"
  auto_store: true  # Automatically store tool results
  default_context: "session"  # Default context for new sessions

# Model Context Protocol (MCP) Configuration
# Phase 1: Foundation - Connect to MCP servers for external tool access
mcp:
  enabled: true
  servers:
    # Filesystem MCP Server - Access files via standardized protocol
    # Requires Node.js and npx installed
    filesystem:
      name: "filesystem"
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "/Users/roderick.vannievelt"]
      enabled: true
      # env:
      #   EXAMPLE_VAR: "${HOME}/path"  # Environment variables (optional)

    # GitHub MCP Server - GitHub API access
    # Requires: GITHUB_TOKEN environment variable
    # Tools: create_issue, list_repos, get_pr, create_pr, etc.
    github:
      name: "github"
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-github"]
      enabled: false  # Set to true and configure GITHUB_TOKEN to enable
      env:
        GITHUB_TOKEN: "${GITHUB_TOKEN}"

    # Postgres MCP Server - Database queries
    # Requires: DATABASE_URL environment variable
    # Tools: query, list_tables, describe_table, etc.
    postgres:
      name: "postgres"
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-postgres"]
      enabled: false  # Set to true and configure DATABASE_URL to enable
      env:
        DATABASE_URL: "${DATABASE_URL}"

    # Slack MCP Server - Slack integration
    # Requires: SLACK_BOT_TOKEN and SLACK_TEAM_ID environment variables
    # Tools: send_message, list_channels, get_channel_history, etc.
    slack:
      name: "slack"
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-slack"]
      enabled: false  # Set to true and configure Slack tokens to enable
      env:
        SLACK_BOT_TOKEN: "${SLACK_BOT_TOKEN}"
        SLACK_TEAM_ID: "${SLACK_TEAM_ID}"

    # Memory MCP Server - Persistent key-value storage
    # No API keys required
    # Tools: store_memory, recall_memory, list_memories, etc.
    memory:
      name: "memory"
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-memory"]
      enabled: false  # Set to true to enable persistent memory

    # Telegram MCP Server - Chat with Wilson from your phone!
    # Requires: TELEGRAM_BOT_TOKEN environment variable
    # Tools: send_message, get_updates, answer_callback_query, etc.
    telegram:
      name: "telegram"
      command: "npx"
      args: ["-y", "@chaindead/mcp-telegram"]
      enabled: false  # Set to true and configure TELEGRAM_BOT_TOKEN to enable
      env:
        TELEGRAM_BOT_TOKEN: "${TELEGRAM_BOT_TOKEN}"
        # Optional: Configure polling interval (default: 1000ms)
        # TELEGRAM_POLLING_INTERVAL: "2000"

    # More MCP servers available at:
    # https://github.com/modelcontextprotocol/servers
    # Community servers: https://github.com/chaindead (Telegram, Discord, etc.)
